{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create balanced datasets\n",
    "The goal of this Jupyter Notebook is to create two balanced datasets that can be utilized for the machine learning models.\n",
    "\n",
    "In the first balanced dataset, the tweets will only be backtranslated. \n",
    "\n",
    "For the second balanced dataset, we will also randomly replaces words in tweet with synonyms.\n",
    "\n",
    "## Import data and create variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle(\"~/Documents/Github Repository/early-warning-twitter/Processed datasets/Tweets/01-06-2020-unique-tweets-amsterdam-demonstration.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate non incident related tweets and incident related tweets\n",
    "not_incident_related_data = data[data[\"has_Incident_Related\"]==0]\n",
    "incident_related_data = data[data[\"has_Incident_Related\"]==1]\n",
    "\n",
    "# Reset the index\n",
    "not_incident_related_data.reset_index(inplace=True, drop=True)\n",
    "incident_related_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtranslation on dataset\n",
    "For every Incident Related tweet, the partly processed tweet will be translated to English and French and translated back to Dutch. If the backtranslated tweet is different than the original tweet, it will be added to the dataset.\n",
    "\n",
    "Back translation will be performed by using the Google Translate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import translate_v2 as translate\n",
    "import time\n",
    "\n",
    "# Get Google API Credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"path-to-json-file\"\n",
    "\n",
    "# Initiate translation Client\n",
    "client = translate.Client()\n",
    "\n",
    "incident_related_back_translated = pd.DataFrame(columns=[\"preprocessed_text_partly\", \"hashtags\", \"has_media\", \"has_Incident_Related\"])\n",
    "\n",
    "for i in range(len(incident_related_data)):\n",
    "    \n",
    "    # Get partly preprocessed text\n",
    "    text = incident_related_data.loc[i, 'preprocessed_text_partly']\n",
    "    \n",
    "    # Translate it to English using Google Translate API\n",
    "    middle = client.translate(text, target_language='en')[\"translatedText\"]\n",
    "    \n",
    "    # Translate it back to Dutch using Google Translate API\n",
    "    output = client.translate(middle, target_language='nl')[\"translatedText\"]\n",
    "    \n",
    "    # Check if the text is not equal to the output\n",
    "    if(text != output):\n",
    "        incident_related_back_translated = incident_related_back_translated.append({\"preprocessed_text_partly\":output, \"hashtags\": incident_related_data.loc[i, 'hashtags'], \"has_media\":incident_related_data.loc[i, 'has_media'], \"has_Incident_Related\":incident_related_data.loc[i, 'has_Incident_Related']}, ignore_index=True)\n",
    "    \n",
    "    # Translate it to French using Google Translate API\n",
    "    middle2 = client.translate(text, target_language='fr')[\"translatedText\"]\n",
    "    \n",
    "    # Translate it back to Dutch using Google Translate API\n",
    "    output2 = client.translate(middle2, target_language='nl')[\"translatedText\"]\n",
    "    \n",
    "    # Check if the text is not equal to the output\n",
    "    if(text != output2) and (output != output2):\n",
    "        incident_related_back_translated = incident_related_back_translated.append({\"preprocessed_text_partly\":output, \"hashtags\": incident_related_data.loc[i, 'hashtags'], \"has_media\":incident_related_data.loc[i, 'has_media'], \"has_Incident_Related\":incident_related_data.loc[i, 'has_Incident_Related']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import translate_v2 as translate\n",
    "import time\n",
    "\n",
    "# Get Google API Credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/Users/jorenwouters/Documents/TranslationAPI/GoogleCloudKey_MyServiceAcct.json\"\n",
    "\n",
    "# Initiate translation Client\n",
    "client = translate.Client()\n",
    "\n",
    "incident_related_back_translated = pd.DataFrame(columns=[\"preprocessed_text_partly\", \"hashtags\", \"has_media\", \"has_Incident_Related\"])\n",
    "\n",
    "for i in range(len(incident_related_data)):\n",
    "    \n",
    "    # Get partly preprocessed text\n",
    "    text = incident_related_data.loc[i, 'preprocessed_text_partly']\n",
    "    \n",
    "    # Translate it to English using Google Translate API\n",
    "    middle = client.translate(text, target_language='en')[\"translatedText\"]\n",
    "    \n",
    "    # Translate it back to Dutch using Google Translate API\n",
    "    output = client.translate(middle, target_language='nl')[\"translatedText\"]\n",
    "    \n",
    "    # Check if the text is not equal to the output\n",
    "    if(text != output):\n",
    "        incident_related_back_translated = incident_related_back_translated.append({\"preprocessed_text_partly\":output, \"hashtags\": incident_related_data.loc[i, 'hashtags'], \"has_media\":incident_related_data.loc[i, 'has_media'], \"has_Incident_Related\":incident_related_data.loc[i, 'has_Incident_Related']}, ignore_index=True)\n",
    "    \n",
    "    # Translate it to French using Google Translate API\n",
    "    middle2 = client.translate(text, target_language='fr')[\"translatedText\"]\n",
    "    \n",
    "    # Translate it back to Dutch using Google Translate API\n",
    "    output2 = client.translate(middle2, target_language='nl')[\"translatedText\"]\n",
    "    \n",
    "    # Check if the text is not equal to the output\n",
    "    if(text != output2) and (output != output2):\n",
    "        incident_related_back_translated = incident_related_back_translated.append({\"preprocessed_text_partly\":output, \"hashtags\": incident_related_data.loc[i, 'hashtags'], \"has_media\":incident_related_data.loc[i, 'has_media'], \"has_Incident_Related\":incident_related_data.loc[i, 'has_Incident_Related']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace random words in tweet with synonyms\n",
    "In each tweet, 3 random words will be replaced with a synonym.\n",
    "To replace words with synonyms, a request to synoniemen.net is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "synonyms = pd.DataFrame(columns=[\"word\", \"synonyms\"])\n",
    "not_synonyms = ['meter']\n",
    "\n",
    "# This function randomly replaces words in a string with synonyms\n",
    "# First parameter is the string itself\n",
    "# Second parameter is the number of words that need to be replaced\n",
    "# Third parameter is a list that contains words we will not replace\n",
    "def random_replace_words_synonyms(text, n_words, exclude_words):\n",
    "    \n",
    "    global synonyms\n",
    "    global not_synonyms\n",
    "    \n",
    "    # Convert the text to a list\n",
    "    list_text = list(text.split(\" \"))\n",
    "    \n",
    "    # Determine the length of the list\n",
    "    length_list = len(list_text)\n",
    "    \n",
    "    if length_list < n_words:\n",
    "        while length_list < n_words:\n",
    "            print(\"This string is larger than the number of words that needs to be replaced!\", text)\n",
    "            n_words = n_words - 1\n",
    "    \n",
    "    list_numbers = []\n",
    "    for i in range(n_words):\n",
    "        \n",
    "        # Try to replace a word with a synonym, until you have a match\n",
    "        while True:\n",
    "            # Choose a random int (based on the maximum number in the list)\n",
    "            random_int = random.randint(0, length_list-1)\n",
    "\n",
    "            # Get the word that corresponds to the random int\n",
    "            word_chosen = list_text[random_int]\n",
    "            \n",
    "            break_twice = False\n",
    "\n",
    "            # Check if the chosen word is not already used\n",
    "            # And check if the chosen word is not one of the excluded words\n",
    "            if (random_int in list_numbers) or (word_chosen in exclude_words) or (word_chosen in not_synonyms):\n",
    "                break;\n",
    "                \n",
    "            # Check if we already know the synonyms for the chosen word\n",
    "            elif (synonyms['word'] == word_chosen).any() == True:\n",
    "                # Get index of the row in the synonyms DataFrame\n",
    "                index_word = synonyms[synonyms[\"word\"]==word_chosen].index.values.astype(int)[0]\n",
    "                \n",
    "                # Get the synonyms for the word\n",
    "                list_synonyms = synonyms.loc[index_word, 'synonyms']\n",
    "                \n",
    "                # Choose a random synonym from the list of synonyms\n",
    "                synonym = list_synonyms[random.randint(0, len(list_synonyms)-1)]\n",
    "                \n",
    "                list_text[random_int] = synonym\n",
    "            \n",
    "            else:\n",
    "                list_numbers.append(random_int)\n",
    "                \n",
    "                # Determine synonym for the chosen word\n",
    "                url = f\"https://synoniemen.net/index.php?zoekterm={word_chosen}\"\n",
    "    \n",
    "                # Make a request to the URL\n",
    "                reqs = requests.get(url)\n",
    "\n",
    "                # Get the HTML of the URL\n",
    "                soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "                # Get the synonyms list element on the page\n",
    "                synonym_list_element = soup.find('dl', {\"class\":\"alstrefwoordtabel\"})\n",
    "                \n",
    "                # If there are synonyms available\n",
    "                if synonym_list_element != None:\n",
    "                    synonym_dd = synonym_list_element.find('dd')\n",
    "                    synonym_a = synonym_dd.find_all('a')\n",
    "                    synonyms_list = []\n",
    "                    \n",
    "                    # Store every synonym in the synonyms_list\n",
    "                    for element in synonym_a:\n",
    "                        synonyms_list.append(element.get_text())\n",
    "                    \n",
    "                    # Append the list of synonyms to the synonyms DataFrame\n",
    "                    synonyms = synonyms.append({\"word\":word_chosen, \"synonyms\":synonyms_list}, ignore_index=True)\n",
    "                    \n",
    "                    # Choose a random synonym from the list of synonyms\n",
    "                    synonym = synonyms_list[random.randint(0, len(synonyms_list)-1)]\n",
    "                    \n",
    "                    list_text[random_int] = synonym\n",
    "                \n",
    "                # If there are no synonyms available\n",
    "                else:\n",
    "                    not_synonyms.append(word_chosen)\n",
    "                    \n",
    "                    # Necessary to break the while statement\n",
    "                    break_twice = True\n",
    "                    continue\n",
    "            \n",
    "            # Break the while statement\n",
    "            if(break_twice==True):\n",
    "                break;\n",
    "    \n",
    "    return list_text\n",
    "\n",
    "incident_related_synonym_replaced = pd.DataFrame(columns=[\"preprocessed_text_partly\", \"hashtags\", \"has_media\", \"has_Incident_Related\"])\n",
    "\n",
    "for i in range(len(incident_related_data)):\n",
    "    \n",
    "    # Get preprocessed text\n",
    "    text = incident_related_data.loc[i, 'preprocessed_text']\n",
    "    \n",
    "    # Replace synonyms\n",
    "    output = random_replace_words_synonyms(text, 3, exclude_words=['halsema', 'at5'])\n",
    "    \n",
    "    # Check if the text is not equal to the output\n",
    "    if(text != output):\n",
    "        incident_related_synonym_replaced = incident_related_synonym_replaced.append({\"preprocessed_text_partly\":output, \"hashtags\": incident_related_data.loc[i, 'hashtags'], \"has_media\":incident_related_data.loc[i, 'has_media'], \"has_Incident_Related\":incident_related_data.loc[i, 'has_Incident_Related']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the synonyms that were used\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the two created datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.39 seconds)\n",
      "Writing emoji data to /Users/jorenwouters/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download demoji codes\n",
    "demoji.download_codes()\n",
    "\n",
    "# Clean the text of the tweet\n",
    "def clean_text(row, variable, hashtag_text='keep', representation = 'string'):\n",
    "    \n",
    "    # Parameters\n",
    "    # hashtag_text, default = 'keep'\n",
    "        # 'keep' - keeps the hashtag text and only removes the '#' in the text\n",
    "        # 'lose' - both removes the hashtag text and the '#' in the text\n",
    "    # representation, default = 'string'\n",
    "        # 'list' - returns a list of words\n",
    "        # 'string' - returns a sentence in string format\n",
    "    \n",
    "    tweet = row[variable]\n",
    "    \n",
    "    # Make the tweet lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove words with less than two characters\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = remove_url(tweet)\n",
    "    \n",
    "    # Remove punctuations unless they are part of a digit (such as \"5.000\")\n",
    "    tweet = re.sub(r'(?:(?<!\\d)[.,;:…‘]|[.,;:…‘](?!\\d))', '', tweet)\n",
    "    \n",
    "    # Remove emojis\n",
    "    tweet = demoji.replace(tweet, \"\")\n",
    "    \n",
    "    if hashtag_text == 'keep':\n",
    "        tweet = tweet.replace(\"#\", \"\")\n",
    "        # Remove mentions (also the text after the @)\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)\", \"\", tweet).split())\n",
    "    else:\n",
    "        # Remove hashtags and mentions (also the text after the # and @)\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \"\", tweet).split())\n",
    "    \n",
    "    # Remove non-alphanumeric charachters, line breaks and tabs\n",
    "    tweet = ' '.join(re.sub(\"([:/\\!@#$%^&*()_+{}[\\];\\\"”\\'|?<>~`\\-\\n\\t’])\", \"\", tweet).split())\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    tweet = word_tokenize(tweet)\n",
    "    \n",
    "    # Use Dutch stop words\n",
    "    stop_words = stopwords.words('dutch') + [\"rt\", \"nan\", \"NaN\"] \n",
    "    \n",
    "    # Remove stopwords\n",
    "    tweet = [w for w in tweet if not w in stop_words]\n",
    "    \n",
    "    if representation == 'list':\n",
    "        return tweet\n",
    "    else:\n",
    "        return listToString(tweet)\n",
    "    \n",
    "# Function to convert a list to a string\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "    \n",
    "def remove_url(tweet_text):\n",
    "    if has_url_regex(tweet_text): \n",
    "        url_regex_list = regex_url_extractor(tweet_text)\n",
    "        for url in url_regex_list:\n",
    "            tweet_text = tweet_text.replace(url, \"\")\n",
    "    return tweet_text\n",
    "\n",
    "def has_url_regex(tweet_text):\n",
    "    return regex_url_extractor(tweet_text)\n",
    "\n",
    "def regex_url_extractor(tweet_text):\n",
    "    return re.findall('https?:\\/\\/(?:[-\\w\\/.]|(?:%[\\da-fA-F]{2}))+', tweet_text)\n",
    "\n",
    "# Clean the tweet texts and put it in four variables per dataset\n",
    "incident_related_back_translated[\"preprocessed_text\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"keep\", \"string\"]), axis=1)\n",
    "incident_related_back_translated[\"preprocessed_text_no_hashtag\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"lose\", \"string\"]), axis=1)\n",
    "incident_related_back_translated[\"preprocessed_text_tokenized\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"keep\", \"list\"]), axis=1)\n",
    "incident_related_back_translated[\"preprocessed_text_tokenized_no_hashtag\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"lose\", \"list\"]), axis=1)\n",
    "\n",
    "incident_related_synonym_replaced[\"preprocessed_text\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"keep\", \"string\"]), axis=1)\n",
    "incident_related_synonym_replaced[\"preprocessed_text_no_hashtag\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"lose\", \"string\"]), axis=1)\n",
    "incident_related_synonym_replaced[\"preprocessed_text_tokenized\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"keep\", \"list\"]), axis=1)\n",
    "incident_related_synonym_replaced[\"preprocessed_text_tokenized_no_hashtag\"] = incident_related_back_translated.apply(clean_text, args=([\"preprocessed_text_partly\", \"lose\", \"list\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create two datasets\n",
    "Now, we will create the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From all the incident related tweets, only get the preprocessed_text_partly, hashtags and has_media\n",
    "incident_related_data_rel = incident_related_data[[\"preprocessed_text\", \"preprocessed_text_no_hashtag\", \"preprocessed_text_tokenized\", \"preprocessed_text_tokenized_no_hashtag\", \"hashtags\", \"has_media\", \"has_Incident_Related\"]]\n",
    "\n",
    "# Merge incident-related tweets and backtranslated incident-related tweets into one dataframe\n",
    "all_incident_related_data = pd.concat([incident_related_back_translated, incident_related_data_rel])\n",
    "\n",
    "# From all the not incident related tweets, only get the preprocessed_text_partly, hashtags and has_media\n",
    "not_incident_related_data_rel = not_incident_related_data[[\"preprocessed_text\", \"preprocessed_text_no_hashtag\", \"preprocessed_text_tokenized\", \"preprocessed_text_tokenized_no_hashtag\", \"hashtags\", \"has_media\", \"has_Incident_Related\"]]\n",
    "\n",
    "# Create the first balanced dataset\n",
    "balanced_data = pd.concat([all_incident_related_data, not_incident_related_data_rel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge incident-related tweets and backtranslated incident-related tweets into one dataframe\n",
    "all_incident_related_data_2 = pd.concat([incident_related_back_translated, incident_related_data_rel, incident_related_synonym_replaced])\n",
    "\n",
    "# Create the second balanced dataset\n",
    "balanced_data_2 = pd.concat([all_incident_related_data_2, not_incident_related_data_rel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data\n",
    "balanced_data.reset_index(inplace=True, drop=True)\n",
    "balanced_data.to_csv(\"~/Documents/Github Repository/early-warning-twitter/Processed datasets/Tweets/01-06-2020-unique-tweets-amsterdam-demonstration-balanced.csv\")\n",
    "balanced_data.to_pickle(\"~/Documents/Github Repository/early-warning-twitter/Processed datasets/Tweets/01-06-2020-unique-tweets-amsterdam-demonstration-balanced.pkl\")\n",
    "\n",
    "balanced_data_2.reset_index(inplace=True, drop=True)\n",
    "balanced_data_2.to_csv(\"~/Documents/Github Repository/early-warning-twitter/Processed datasets/Tweets/01-06-2020-unique-tweets-amsterdam-demonstration-balanced-2.csv\")\n",
    "balanced_data_2.to_pickle(\"~/Documents/Github Repository/early-warning-twitter/Processed datasets/Tweets/01-06-2020-unique-tweets-amsterdam-demonstration-balanced-2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6978"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5070\n",
       "1    1429\n",
       "Name: has_Incident_Related, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data[\"has_Incident_Related\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5070\n",
       "1    1908\n",
       "Name: has_Incident_Related, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data_2[\"has_Incident_Related\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
